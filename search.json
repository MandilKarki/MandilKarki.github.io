[
  {
    "objectID": "posts/Mistral/mistral_long.html",
    "href": "posts/Mistral/mistral_long.html",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "",
    "text": "Attention mechanism is a key component in Transformer models. It allows the model to focus on different parts of the input sequence and derive the relationship between tokens of the input sequence. In other words, Attention refers to the mechanism of sharing the information between tokens in the input sequence. For the computation of attention in Transormer Models:\n\nWe multiply the query matrix with the transpose of the key matrix to get the attention scores. The attention scores are then fed to the softmax function to get the attention weights. These attention weights are then multiplied with the value matrix to get the context vector or attention output.\n\nIn vanilla Transformer models, the attention mechanism uses causal mask which means that each token in the input sequence can only attend to itself and all the tokens before it. This approach ensures that the model is causal and it can only use the information from the past tokens to predict the future tokens. However, the number of operations in the attention mechanism is quadratic with respect to the length of the input sequence and the memory requirement is also linear with respect to the length of the input sequence which incurs higher latency and smaller throughput from the model at inference time.\n\n\n\nFig a. Vanilla Attention with causal mask taken from Mistral paper.\n\n\nSo, to address these limitations, Sliding Window Attention mechanism was used in the Mistral model. The Sliding Window Attention is a variant of the attention mechanism which uses a fixed window size to select the tokens from the past to attend for computation of attention output. In other words, each token in the input sequence can only attend at most W tokens from the past where W is the window size. This approach increases the inference speed and reduces the memory usage of the model. This mechanism still ensures that the model is causal but it does not use the entire tokens from the past, whereas it uses a fixed number of tokens from the past to compute the attention output.\n\n\n\nFig b. Sliding Window Attention with window size 3.\n\n\nIn the Sliding Window Attention, the tokens outside the sliding window still influence the next word prediction because at each attention layer, the information can move forward by W tokens at most, but after the next layer, the information can move forward by 2W tokens and so on, as the hidden state in position i of the layer k, attends to all the hidden states from position i-W to i of the layer k-1 since the layers are stacked on top of each other in Transformer models.\nSuppose, we have a input sequence with 10 tokens, a Transformer model with 4 layers and using a window size of 4,then the information flow from one layer to another using Sliding Window Attention is given below:\n\nIn first layer, we use the first 4 tokens to compute the attention output for the next token.\nIn second layer, we use the information from the previous 4 tokens and the next 3 tokens to compute the attention output for the next token.\nIn third layer, we use the information from the previous tokens and the next 3 tokens compute the attention output for the next token.\nIn fourth layer, we use the information of the entire input sequence to compute the attention output for the next token. At this point, the information has propagated from the first token to the last token in the input sequence.\n\n\n\n\nFig c. Information flow from one layer to another using Sliding Window Attention."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#sliding-window-attention",
    "href": "posts/Mistral/mistral_long.html#sliding-window-attention",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "",
    "text": "Attention mechanism is a key component in Transformer models. It allows the model to focus on different parts of the input sequence and derive the relationship between tokens of the input sequence. In other words, Attention refers to the mechanism of sharing the information between tokens in the input sequence. For the computation of attention in Transormer Models:\n\nWe multiply the query matrix with the transpose of the key matrix to get the attention scores. The attention scores are then fed to the softmax function to get the attention weights. These attention weights are then multiplied with the value matrix to get the context vector or attention output.\n\nIn vanilla Transformer models, the attention mechanism uses causal mask which means that each token in the input sequence can only attend to itself and all the tokens before it. This approach ensures that the model is causal and it can only use the information from the past tokens to predict the future tokens. However, the number of operations in the attention mechanism is quadratic with respect to the length of the input sequence and the memory requirement is also linear with respect to the length of the input sequence which incurs higher latency and smaller throughput from the model at inference time.\n\n\n\nFig a. Vanilla Attention with causal mask taken from Mistral paper.\n\n\nSo, to address these limitations, Sliding Window Attention mechanism was used in the Mistral model. The Sliding Window Attention is a variant of the attention mechanism which uses a fixed window size to select the tokens from the past to attend for computation of attention output. In other words, each token in the input sequence can only attend at most W tokens from the past where W is the window size. This approach increases the inference speed and reduces the memory usage of the model. This mechanism still ensures that the model is causal but it does not use the entire tokens from the past, whereas it uses a fixed number of tokens from the past to compute the attention output.\n\n\n\nFig b. Sliding Window Attention with window size 3.\n\n\nIn the Sliding Window Attention, the tokens outside the sliding window still influence the next word prediction because at each attention layer, the information can move forward by W tokens at most, but after the next layer, the information can move forward by 2W tokens and so on, as the hidden state in position i of the layer k, attends to all the hidden states from position i-W to i of the layer k-1 since the layers are stacked on top of each other in Transformer models.\nSuppose, we have a input sequence with 10 tokens, a Transformer model with 4 layers and using a window size of 4,then the information flow from one layer to another using Sliding Window Attention is given below:\n\nIn first layer, we use the first 4 tokens to compute the attention output for the next token.\nIn second layer, we use the information from the previous 4 tokens and the next 3 tokens to compute the attention output for the next token.\nIn third layer, we use the information from the previous tokens and the next 3 tokens compute the attention output for the next token.\nIn fourth layer, we use the information of the entire input sequence to compute the attention output for the next token. At this point, the information has propagated from the first token to the last token in the input sequence.\n\n\n\n\nFig c. Information flow from one layer to another using Sliding Window Attention."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#rotating-buffer-cache",
    "href": "posts/Mistral/mistral_long.html#rotating-buffer-cache",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Rotating Buffer Cache",
    "text": "Rotating Buffer Cache\nRotating Buffer Cache is a mechanism used in the Mistral model which limits the size of KV cache to a fixed size. In my blog post on Understanding KV Cache, I have explained the needs and limitations of KV cache along with implementation. In the paper of Mistral, they mentioned that: when we use a sequence length of 32k tokens, the Rotating Buffer Cache reduces the cache memory usage by 8x, without impacting the model quality. While using KV cache, the memory usage of GPU or CPU increases linearly with respect to the length of the input sequence because we need to save KV cache for each layers of the model.\n\nRotating Buffer Cache for Sliding Window Attention\nWhen we use Sliding Window Attention mechanism, we have the fixed attention span of W tokens which means that we can limit the size of the KV cache using the Rotating Buffer Cache also called Rolling Buffer Cache. The cache has a fixed size of W where W is the window size, and the Keys and Values for the timestep i are stored in the positions i mod W of the cache. Similarly, when the position i is larger than W, the previous Keys and Values are overwritten by the new Keys and Values in the cache. This approach keeps the cache size fixed and reduces the linear memory usage of the model with respect to the length of the input sequence.\n\n\n\nFig d. Rotating Buffer Cache with cache size W=4\n\n\nIn the above figure, the cache size is W=4 and we have 3 input sequences. The Keys and Values are stored by following operations:\n\nWe have 3 input sequences:\n\n“This is an example of …”\n“Mistral is a good …”\n“The cat sat on the mat …”\n\nFor the first input sequence, at timestep i, “an” is stored in the cache position 2 mod 4 = 2. At timestep i+1, “example” is stored in the cache position 3 mod 4 = 3. At timestep i+2, “of” is stored in the cache position 4 mod 4 = 0.\nFor the second input sequence, at timestep i, “is” is stored in the cache position 1 mod 4 = 1. At timestep i+1, “a” is stored in the cache position 2 mod 4 = 2. At timestep i+2, “good” is stored in the cache position 3 mod 4 = 3.\nFor the third input sequence, at timestep i, “on” is stored in the cache position 3 mod 4 = 3. At timestep i+1, “the” is stored in the cache position 4 mod 4 = 0. At timestep i+2, “mat” is stored in the cache position 1 mod 4 = 1."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#pre-filling-and-chunking",
    "href": "posts/Mistral/mistral_long.html#pre-filling-and-chunking",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Pre-Filling and Chunking",
    "text": "Pre-Filling and Chunking\nWhen we generate the sequence using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. The output of the model when we use self attention mechanism without KV cache is a sequence of tokens whereas the output of the model when we use KV cache is a required final token. Therefore, KV cache makes the inference faster.\nAt inference, the prompt sequence is known in advance, and we can pre-fill the KV cache with the prompt sequence. In the Sliding Window Attention mechanism, we have the fixed size of the cache and if the prompt sequence is larger than the cache size, we can chunk the prompt sequence into smaller sequences and pre-fill the cache with these smaller sequences. In this approach, we use the chunk size of W where W is the window size.\nSuppose, we have a prompt sequence “The cat sat on the mat and saw the dog go to” and the window size is W=4, then we can chunk the prompt sequence into smaller sequences as follows: - “The cat sat on” - “the mat and saw” - “the dog go to”\n\n\n\nFig e. Pre-fill and Chunking representation taken from Mistral\n\n\nIn the above figure, the prompt sequence is chunked into smaller sequences and the cache is pre-filled with these sequences. For the third chunk “the dog go to”: it attends itself using the causal mask which is the rightmost block, and it attends the previous block using the sliding window attention mechanism, and it does not attend any past tokens that is outside the window size of sliding window attention.\nTherefore, pre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences.\n\nBlockDiagonalCausalMask and BlockDiagonalMask during Pre-Filling\nWe use the Pre-Filling and Chunking mechanism to pre-fill the KV cache with the prompt sequence. Both the BlockDiagonalCausalMask and BlockDiagonalMask are used to pre-fill the cache and generate the mask of each chunk of the prompt sequence. The generated mask is then used to compute the attention by the Attention Layer in Mistral model. The generation of the mask is done by the following operations:\n\nLet’s say we have a prompt sequence “The cat sat on” and we want to generate the 4 new tokens. The cache size will be max_tokens + prompt_length + 1 = 4 + 4 + 1 = 9.\nSuppose, we consider the chunk size of 2, then the prompt sequence is divided into 3 chunks because while encoding the prompt sequence, we need to add the start token at index 0 during the inference, so the prompt sequence is divided into 3 chunks: “[start token] The,”cat sat”, “on”. You can find the implementation of chunking in here.\nSo, the prefill or mask for the first chunk is generated by BlockDiagonalCausalMask and the prefill or mask for the second (or subsequent chunks if any) is generated by BlockDiagonalMask. In our example above, we have the last chunk that doesn’t contain the complete tokens as the chunk size has only one token, and in such cases, the prefill or mask is generated by BlockDiagonalCausalWithOffsetPaddedKeysMask.\n\n\n\n\nFig. Mask sample generated by BlockDiagonalCausalMask.\n\n\n\n\n\nFig. Mask sample generated by BlockDiagonalMask during Pre-filling phase.\n\n\nSo, the BlockDiagonalCausalMask is used to pre-fill the cache with the first chunk of the prompt sequence and the BlockDiagonalMask is used to pre-fill the cache with the subsequent chunks of the prompt sequence. You can find the implementation of these masks in the cache.py."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation",
    "href": "posts/Mistral/mistral_long.html#blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "BlockDiagonalCausalWithOffsetPaddedKeysMask for Token Generation",
    "text": "BlockDiagonalCausalWithOffsetPaddedKeysMask for Token Generation\nWhen we generate the tokens using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. In Mistral, the BlockDiagonalCausalWithOffsetPaddedKeysMask is used to generate the mask when we feed a single token to the model to generate the next token. Therefore, the BlockDiagonalCausalWithOffsetPaddedKeysMask is used to generate the mask for the token generation. You can find the implementation of generating a new token in here which then uses the BlockDiagonalCausalWithOffsetPaddedKeysMask to generate the mask for the generation of the next token.\n\nHow multiple prompts are handled in Mistral?\nIn Mistral model, multiple prompts are packed into a single tensor during pre-filling phase at inference time and the corresponding mask is generated using the BlockDiagonalCausalMask , BlockDiagonalMask and BlockDiagonalCausalWithOffsetPaddedKeysMask. The packed tensor is then send to the model to pre-fill the cache and generate the mask for the token generation. You can find the implementation of packing the prompts into a single tensor and generating the mask in here."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#kv-cache",
    "href": "posts/Mistral/mistral_long.html#kv-cache",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "KV Cache",
    "text": "KV Cache\nKV cache, short for Key & Value cache, is a technique used to accelerate the inference process in Large Language Models (LLMs), particularly in autoregressive models i.e. the current token depends on the previous tokens in a sequence. In the KV cache, the output of the model from previous time step is appended to the cache of key and value matrices of the current time step but the query matrix is updated at each time step to generate the next token. This way of caching the previous keys and values ensures that the model does not repeat the computations at each time step. This significantly reduces the size of the matrices used in the computation which makes the inference process faster (matrix multiplication faster). I have explained KV cache in detail along with its implementation in my blog post on Understanding KV Cache.\n\nKV Cache in Pre-Filling and Chunking\nPre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the long prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences as explained above. Since, we divide the prompt sequence into smaller sequences or chunks to pre-fill the cache, the dimensions of the KV cache changes at each iteration of the pre-filling process. The complete operation of pre-filling is done by the following operations:\n\nLet’s say we have a prompt sequence “This is another great test” and after encoding we will have encoded prompt sequence of length 6 because we need to add the start token at index 0 during the inference.\nSuppose, we consider the chunk size of 2, then the prompt sequence is divided into 3 chunks: “[start token] This”, “is another”, “great test”.\nFor the first chunk, the KV cache sequence length is 0, and since the chunk size is 2, the KV cache dimension is [2, 8, 128] where 2 is sequence length, 8 is the number of KV heads, and 128 is the head dimension.\nFor the second chunk, the KV cache sequence length is 2, and since the chunk size is 2, the KV cache dimension now becomes [4, 8, 128] where 4 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.\nFor the third and last chunk, the KV cache sequence length is 4, and since the chunk size is 2, the KV cache dimension now becomes [6, 8, 128] where 6 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.\n\nSo, the KV cache is pre-filled with the prompt sequence and the dimensions of the KV cache changes at each iteration of the pre-filling process. You can check the dimensions of the KV cache at each iteration of the pre-filling process in the model.py."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#conclusion",
    "href": "posts/Mistral/mistral_long.html#conclusion",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I have explained the Sliding Window Attention mechanism, Rotating Buffer Cache, Pre-Filling and Chunking, BlockDiagonalCausalMask, BlockDiagonalMask, BlockDiagonalCausalWithOffsetPaddedKeysMask, and KV Cache in Mistral model. These techniques are used to accelerate the inference process as well as fine-tuning process in Large Language Models (LLMs). We prepare this blog post based on the source code of Mistral model and the paper of Mistral. You can learn more novel techniques used in Mistral model by reading my previous blog post on Model Sharding and Mixture of Experts."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#references",
    "href": "posts/Mistral/mistral_long.html#references",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "References",
    "text": "References\n\nMistral Source Code\nLlama2 Implementation\nAttention Is All You Need"
  },
  {
    "objectID": "posts/Dataset/DataPreparation.html",
    "href": "posts/Dataset/DataPreparation.html",
    "title": "Data Fundamentals",
    "section": "",
    "text": "Outliers\nOutliers are examples that look dissimilar to the majority of examples from the dataset. Dissimilarity is measured by some distance metric, such as Euclidean distance. Deleting outliers from the training dataset is not considered scientifically significant, especially in small datasets. In the big data context, outliers don’t typically have a significant impact on the model.\n\n\nData Leakage\nData leakage, also known as target leakage, is a problem affecting several stages of the machine learning life cycle, from data collection to model evaluation. Data leakage in supervised learning is the unintentional introduction of information about the target that shouldn’t be made available.\n\n\nWhat is Good Data\n\nGood data is informative.\nGood data has good coverage.\nGood data reflects real inputs.\nGood data is unbiased.\nGood data is not a result of a feedback loop.\nGood data has consistent labels.\nGood data is big enough to allow generalization.\n\n\n\nData Augmentation\nThe most effective strategy applied to images to get more labeled examples without additional labeling is called data augmentation. The simple operations are flip, rotation, crop, color shift, noise addition, perspective correction, contrast change, and information loss.\nMixup is the popular technique of data augmentation which consists of training the model on a mix of the images from the training set. Instead of training the model on the raw images, we take two images and use for training their linear combination:\nmixup_image = t x image₁ + (1 - t) x image₂\nmixup_target = t x target₁ + (1 - t) x target₂\n\n\nOversampling\nOversampling is a technique to mitigate the class imbalance by making multiple copies of minority class examples. Two popular algorithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\n\n\nUndersampling\nUndersampling is a technique to mitigate the class imbalance by removing some examples from the training set of the majority class based on the property called Tomek links. A Tomek link exists between two examples xi and xj belonging to two classes if there’s no other examples xk in the dataset closer to either xi or xj than the latter two are to each other.\n\n\nData Sampling\nIn probability sampling, all examples have a chance of being selected and it involves randomness. Nonprobability sampling is not random and it follows a fixed deterministic sequence of heuristic actions which means that some examples don’t have a chance of being selected, no matter how many samples we build.\nIn stratified sampling, we first divide our dataset into groups called strata and then randomly select examples from each stratum, like in simple random sampling. It often improves the representativeness of the sample by reducing its bias.\n\n\nData Versioning\nIf data is held and updated in multiple places, we might need to keep track of versions. Versioning the data is also needed if we frequently update the model by collecting more data, especially in an automated way. Data versioning can be implemented in several levels of complexity.\nLevel 0: data is unversioned. Level 1: data is versioned as a snapshot at training time. Level 2: both data and code are versioned as one asset. Level 3: using or building a specialized data versioning solution.\n\n\nData Lake\nA data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or files. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data."
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Mandil’s Blog",
    "section": "",
    "text": "Mandil’s Blog\nPlease visit my blog: https://MandilKarki.github.io/blog/\nI am a data science, machine Learning, and deep learning practitioner and learner. Solving a problem and gaining insights with the help of machine learning algorithms have always seemed to be superpowers for me. I am here to solve things, Learning a lot in the process. I’m currently working as a Machine Learning Engineer at\n\nPublication:\n\n\nExperience:\n\nNuminous Labs | Researcher and Developer | Dec 2022 - Present\nRBC Canada | Data Scientist | Feb 2022 - May 2022\nRippeyAI | NLP Team Lead | May 2022 - Dec 2022\nTreeleaf | NLP Engineer | Nov 2020 - Feb 2022\nInspiring Labs | AI Engineer | Apr 2021 - Aug 2021\n\nCommands to publish:\ngit checkout main\nquarto render\ngit push\n\nquarto publish gh-pages"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "Comprehensive Understanding of Mistral Model\n\n\n\n\n\n\nMixture of Experts\n\n\nMistral\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nMandil Karki\n\n\n\n\n\n\n\n\n\n\n\n\nData Fundamentals\n\n\n\n\n\n\nmachine learning\n\n\ndata preparation\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nMandil Karki\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Attention & Transformer\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\nMandil Karki\n\n\n\n\n\n\n\n\n\n\n\n\nWord Vectors\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nMandil Karki\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mandil Karki",
    "section": "",
    "text": "Experienced Machine Learning Scientist with 4+ years of proven expertise in natural language processing, machine learning, deep learning, and data science. Passionate about leveraging artificial intelligence and machine learning to drive innovation, solve complex problems, and gain valuable insights.\nSolving problems and gaining insights with the help of machine learning algorithms have always felt like superpowers to me. I am here to apply my skills and knowledge to tackle challenges head-on, continuously learning and growing in the process.\n\nPublication:\n\n\nExperience:\n\nNuminous Labs | Researcher and Developer | Dec 2022 - Present\nRBC Canada | Data Scientist | Feb 2022 - May 2022\nRippeyAI | NLP Team Lead | May 2022 - Dec 2022\nTreeleaf | NLP Engineer | Nov 2020 - Feb 2022\nInspiring Labs | AI Engineer | Apr 2021 - Aug 2021"
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#word-vectors",
    "href": "posts/WordVectors/WordVectors.html#word-vectors",
    "title": "Word Vectors",
    "section": "Word Vectors",
    "text": "Word Vectors\nWord vectors are also called word embeddings or neural word representations because these whole bunch of words are represented in a high dimensional vector space and they are embedded into that space. They are also called as a distibuted representation.\nWord vectors means having a vector for each word type i.e both for context and outside, which are initialized randomly and those vectors are progressively updated by using iterative algorithms so that they can do better job at predicting which words appear in the context of other words.\n\nDistributional Semantics\nIt states that a word’s meaning is given by the words that frequently appear close-by. When a word w appears in a text, it’s context is the set of words that appear nearby within a fixed-size window."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#word2vec",
    "href": "posts/WordVectors/WordVectors.html#word2vec",
    "title": "Word Vectors",
    "section": "Word2Vec",
    "text": "Word2Vec\nWord2Vec is a framework for learning word vectors developed by Mikolov et al. 2013. The idea behind Word2Vec is that we have a large corpus or body of text and every word in a fixed-vocabulary is represented by a vector. We have to go through each position t in the text, which has a center word c and context words o and we have to use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa. We have to keep adjusting the word vectors to maximize this probability. Word2Vec model maximizes the objective function by putting similar words nearby in high dimensional vector space.\nTwo model variants: - Skip Grams: Predict context words given center word. - Continuous Bag of Words: Predict center word from context words.\nMain Idea of Word2Vec - Start with random word vectors. - Iterate through each word in the whole corpus. - Try to predict the surrounding words using word vectors. Try and predict what words surrounds the center word by using the probability distribution that is defined in terms of the dot product between the word vectors for the center word and the context words. - Updating the vectors so that they can predict the actual surrounding words better and better.\nKey Points: - Word2Vec model actually ignores the position of words. - Taking a log likelihood turns all of the products into sums which decreases the computational complexity. - A dot product is a natural measure for similarity between words. If two words have a larger dot product, that means they are more similar. - The simple way to avoid negative probabilities is to apply exponential function.\nTraining Methods: - To train a model, we gradually adjust parameters to minimize a loss. - Theta represents all the model parameters, in one long vector. We optimize these parameters by walking down the gradient.\nBag of Words Model: - Bag of words models are the models that don’t pay attention to words order or position, it doesn’t matter if the word is near to the center word or a bit further away on the left or right. The probability estimation will be the same at each position.\nIn bag of words, we have outside word vector and center word vector, which undergoes dot product followed by softmax activation function.\n\nOptimization: Gradient Descent\n\nTo learn good word vectors, we have a cost function J(\\(\\theta\\)).\nGradient descent is an algorithm to minimize the J(\\(\\theta\\)) by changing \\(\\theta\\).\nGradient Descent is an iterative learning algorithm that learns to maximize the J(\\(\\theta\\)) by changing the \\(\\theta\\).\nFrom the current value of \\(\\theta\\), calculate the gradient of J(\\(\\theta\\)), then take small step in the direction of negative gradient to gradually move down towards the minimum.\n\nProblems of Gradient Descent - J(\\(\\theta\\)) is a function of all windows in the corpus which is often billions. So, actually working out J(\\(\\theta\\)) or the gradient of J(\\(\\theta\\)) would be extremely expensive because we have to iterate over the entire corpus. - We would wait a very long before making a single update.\n\n\nStochastic Gradient Descent\nStochastic gradient descent is a very simple modification of the gradient descent algorithm. Rather than working out an estimate of the gradient based on the entire corpus, we simply take one center word or a small batch like 32 center words, and we work out the estimate of the gradient based on them.\nStochastic gradient descent is kind of noisy and bounces around as it makes progress, it actually means that in complex networks it learns better solution. So, it can do much more quickly and much better.\n\n\nSoftmax Function\nThe softmax function will take any R in vector and turns it into the range 0 and 1. The name of the softmax function comes from the fact that it’s sort of like a max because the exponential function gives more emphasis to the big contents in different dimensions of calculating the similarity. The softmax function takes some numbers and returns the whole probability distribution.\n\n\nCo-occurence Vector\n\nVectors increase in size with the vocabulary.\nVery high dimensional and requires a lot of storage though sparse.\nSubsequent classification models have sparsity issues which makes models less robust."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#glove",
    "href": "posts/WordVectors/WordVectors.html#glove",
    "title": "Word Vectors",
    "section": "GloVe",
    "text": "GloVe\n\nFast training.\nScalable to huge corpus.\nGood performance even with small corpus and small vectors.\n\nGloVe model unify the thinking between the co-occurence matrix models and the neural models by being someway similar to the neural models but actually calculated on top of a co-occurence matrix count."
  },
  {
    "objectID": "posts/Attention/Transformer.html#recurrent-models",
    "href": "posts/Attention/Transformer.html#recurrent-models",
    "title": "Self-Attention & Transformer",
    "section": "Recurrent Models",
    "text": "Recurrent Models\n\nThe de facto strategy in Natural Language Processing (NLP) is to encode sentences with a bidirectional LSTM models. For example: Source sentence in a translation.\nWe should define our output (parse, sentence, summary) as a sequence, and use an LSTm model to generate it.\nUse of attention mechanism allows flexible access to the memory. We use attention mechanism to take a representation from our decoder and look back to treat the encoded representation as a memory, that we can reference and pick out what’s important to any given time.\n\n\nIssues with Recurrent Models\n\nLinear interaction distance.\n\nRecurrent Neural Networks (RNN) are unrolled left-to-right.\nRNNs encode linear locality which means the nearby words often affect each other’s meaning in the sentence.\nRNNs take O(sequence length) steps for distant words pairs to interact which means that it is hard to learn long-distance dependencies because of the gradient problems.\nLinear order of words is sort of baked into the model because we have to unroll the RNN throughout the sequence and linear order isn’t the right way to think about sentences.\n\nLack of parallelizability.\n\nForward and backward passes have O(sequence length) unparallelizable operations.\nThough GPUs can perform a bunch of independent computations at once, a future RNN hidden states can’t be computed in full before past RNN hidden states have been computed.\n\nSo, RNNs inhibits training on very large dataset."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-windows-model",
    "href": "posts/Attention/Transformer.html#word-windows-model",
    "title": "Self-Attention & Transformer",
    "section": "Word Windows Model",
    "text": "Word Windows Model\n\nWord window models aggregate local context. Number of unparallelizable operations doesn’t increase sequence length.\nStacking word window layers allows interaction between farther words.\nMaximum interaction distance = sequence length / window size. But if the sentences are too long, we will just ignore the long-distance context."
  },
  {
    "objectID": "posts/Attention/Transformer.html#attention-model",
    "href": "posts/Attention/Transformer.html#attention-model",
    "title": "Self-Attention & Transformer",
    "section": "Attention Model",
    "text": "Attention Model\n\nAttention model treats each word’s representation as a query to access and incorporate information for a set of values. Example: In a machine translation system, the set of values were all of the encoder states for the source sentences.\nNumber of unparallelizable operations doesn’t increase sequence length.\nMaximum interaction distance is O(1) since all the wrods interact at every layer.\n\n\nSelf-Attention Model\n\nAttention model operates on queries, keys, and values.\nIn self-attention models, the queries, keys, and values are drawn from the same source sentences.\nSince, self-attention mechanism doesn’t build in order information, we need to encode the order of the sentences in our keys, queries, and values. We will consider representing each sequence index as a vector and add it to our inputs in *self-attention** block.\nThe position representation vectors are represented through sinusoids. Sinusoidal position representations concatenate functions of varying periods. Learned absolute position representations are flexible to be learned to fit the data on each position.\n\n\n\nBarriers & Solutions for Self-Attention Model\n\n\n\n\n\n\n\nBarriers\nSolution\n\n\n\n\n1. Doesn’t have an inherent notion of order.\n1. Add position representations to the inputs.\n\n\n2. No nolinearities to produce the deep learning magic. But it’s all just the weighted averages.\n2. Apply the same feedforward networks to each self-attention output.\n\n\n3. Need to ensure that we don’t look at the future outputs when predicting a sequence. Like in machine translation or language modeling.\n3. Mask out the future by artificially setting the attention weights to 0.\n\n\n\nThe necessities for a self-attention model are as follows:\n\nSelf-attention:\n\nThe basis of the method or implementation process.\n\nPosition representations:\n\nSpecify the sequence order, since self-attention is an unordered function of its inputs.\n\nNonlinearities:\n\nAt the output of the self-attention block.\nFrequently implemented as a simple feedforward network.\n\nMasking:\n\nIn order to parallelize operations while not looking at the future.\nKeeps information about the future from leaking to the past."
  },
  {
    "objectID": "posts/Attention/Transformer.html#the-transformer",
    "href": "posts/Attention/Transformer.html#the-transformer",
    "title": "Self-Attention & Transformer",
    "section": "The Transformer",
    "text": "The Transformer\n\nWe take the dot product of the query-key in one matrix multiplication.\nThen we apply softmax and compute the weighted average with another matrix multiplication.\nWe define multiple attention heads through multiple query, key, and value matrices.\nResidual connections are thought to make the loss landscape considerably smoother and thus enhances easier training.\nLayer normalization is a trick to help models train faster. It cuts down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer.\nScaled Dot Product attention is necessary when dimensionality d becomes large, dot products between vectors tend to become large and because of this, inputs to the softmax function can be large, making the gradients small.\nTransformers parallelizability allows for efficient pretraining, and have made them the de facto standard."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "href": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "title": "Self-Attention & Transformer",
    "section": "Word Structure and Subword Models",
    "text": "Word Structure and Subword Models\n\nIn a language’s vocabulary, we assume that a fixed vocabulary of tens of thousands of words are built from the training dataset. All other novel words which are seen only at test time are mapped to a single unknown token UNK.\nFinite vocabulary assumptions in not an ideal solution in many languages. Many languages exhibit complex morphology or word structure which is more word types, each occurring in fewer times."
  },
  {
    "objectID": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "href": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "title": "Self-Attention & Transformer",
    "section": "Byte-Pair Encoding Algorithm",
    "text": "Byte-Pair Encoding Algorithm\nSubword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level. The dominant modern paradigm is to learn a vocabulary of parts of words also known as subword tokens. At the training and testing time, each word is split into a sequence of known subwords.\nByte-pair encoding is a simple, and effective startegy for defining a subword vocabulary: - Start with a vocabulary containing only characters and an end-of-word symbol. - Using a corpus of text, find the most common adjacent characters known as subwords. - Replace instances of the character pair with the new subword and iterate until the desired vocab size is met.\nThis technique was originally used in NLP for machine translation, and now a similar method WordPiece is used in pretrained models.\n\nPretrained Word Embeddings & Models\n\nAlmost all parameters in NLP networks are initialized via pretraining which is similar to initializing the Word2Vec parameters.\nThe pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts.\nThis has been exceptionally effective at building strong:\n\nrepresentations of language.\nparameter initializations for strong NLP models.\nprobability distributions over language that we can sample from.\n\n\n\n\nGenerative Pretrained Transformer (GPT)\n\nGPT is a decoder only Transformer model with 12 layers.\nGPT contains 768 dimensional hidden states, and 3072 dimensional feed-forward hidden layers.\nA subword vocabulary called Byte-Pair encoding with 40,000 merges.\nGPT models are trained on book corpus and contains over 7000 unique books which contains long spans of contiguous text, for learning long-distance dependencies.\n\n\n\nBidirectional Encoder Representations from Transformers (BERT)\nDevlin et al., 2018 proposed the Masked LM objective and released the weights of a pretrained Transformer and labeled BERT.\nSome of the details about Masked Language Model for BERT are: - Predict a random 15% of subword tokens. - Replace input word with [MASK] 80% of the time. - Replace input word with a random vocabulary token 10% of the time. - Leave input word unchanged 10% of the time but still predict.\nSome of the details about BERT: - Two models were released: - BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params. - BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params. - Trained on: - Books Corpus (800 million words) - English Wikipedia (2500 million words) - Pretraining is expensive and impractical on a single GPU: - BERT was pretrained with 64 TPU chips for a total of 4 days. - TPU are special tensor operations acceleration hardware. - Finetuning is practical and common on a single GPU."
  }
]